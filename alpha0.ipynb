{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(0)\n",
    "from tqdm.notebook import trange\n",
    "import random\n",
    "import math\n",
    "import chess\n",
    "import chess.engine\n",
    "import matplotlib as plt \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 4080 Laptop GPU is available.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "else:\n",
    "    print(\"No GPU available. Training will run on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chess Game**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import numpy as np\n",
    "\n",
    "class ChessGame:\n",
    "    def __init__(self):\n",
    "        self.board = chess.Board()\n",
    "        self.action_size = 1000000  # Max possible moves incl. promotions\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        return self.board.fen()  # FEN is a standard notation for representing the current state of the chess board\n",
    "    \n",
    "    def get_next_state(self, state, action, player):\n",
    "        \"\"\"Update the board with the chosen action.\"\"\" \n",
    "        board = chess.Board(state)\n",
    "        move = chess.Move.from_uci(action)\n",
    "        board.push(move)\n",
    "        return board.fen()\n",
    "    \n",
    "    def get_valid_moves(self, state):\n",
    "        \"\"\"Get a binary mask of valid moves.\"\"\" \n",
    "        board = chess.Board(state)\n",
    "        legal_moves = list(board.legal_moves)\n",
    "        valid_moves = np.zeros(self.action_size, dtype=np.uint8)\n",
    "        for move in legal_moves:\n",
    "            move_idx = self.move_to_index(move)\n",
    "            valid_moves[move_idx] = 1\n",
    "        return valid_moves\n",
    "    \n",
    "    def get_opponent(self, player):\n",
    "        \"\"\"Toggle between players (1 for white, -1 for black).\"\"\"\n",
    "        return -player\n",
    "    \n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "\n",
    "    def check_win(self, state, action):\n",
    "        board = chess.Board(state)\n",
    "        return board.is_game_over()\n",
    "    \n",
    "    def get_value_and_terminated(self, state, action):\n",
    "        board = chess.Board(state)\n",
    "        if board.is_checkmate():\n",
    "            return 1, True\n",
    "        elif board.is_stalemate() or board.is_insufficient_material():\n",
    "            return 0, True\n",
    "        return 0, False\n",
    "    \n",
    "    def get_encoded_state(self, state):\n",
    "        \"\"\"Encode the board state into a tensor format for input to the neural network.\"\"\"\n",
    "        board = chess.Board(state)\n",
    "        board_tensor = np.zeros((13, 8, 8), dtype=np.float32)  # Shape [13, 8, 8]\n",
    "        piece_map = board.piece_map()\n",
    "        \n",
    "        for square, piece in piece_map.items():\n",
    "            piece_type = piece.piece_type - 1 if piece.color else piece.piece_type + 5 \n",
    "            board_tensor[piece_type, square // 8, square % 8] = 1 \n",
    "            \n",
    "        # Set empty squares to the 12th channel\n",
    "        for row in range(8):\n",
    "            for col in range(8):\n",
    "                if board_tensor[:, row, col].sum() == 0:\n",
    "                    board_tensor[12, row, col] = 1  # Mark as empty\n",
    "        \n",
    "        # Convert to torch tensor and move to device\n",
    "        board_tensor = torch.FloatTensor(board_tensor).to(device)\n",
    "    \n",
    "        return board_tensor\n",
    "\n",
    "    \n",
    "    def move_to_index(self, move):\n",
    "        \"\"\"Convert a move to a unique index for the action space.\"\"\" \n",
    "        uci_move = move.uci()\n",
    "        from_square = chess.SQUARE_NAMES.index(uci_move[:2])\n",
    "        to_square = chess.SQUARE_NAMES.index(uci_move[2:4])\n",
    "        promotion = move.promotion or 0\n",
    "        return from_square * 64 + to_square + promotion * 64 * 64\n",
    "\n",
    "    def index_to_move(self, index):\n",
    "        \"\"\"Convert an index back to a move.\"\"\" \n",
    "        promotion = index // (64 * 64)\n",
    "        index %= (64 * 64)\n",
    "        from_square = index // 64\n",
    "        to_square = index % 64\n",
    "        move = chess.Move(from_square, to_square, promotion)\n",
    "        return move.uci()\n",
    "    \n",
    "    def change_perspective(self, state, player):\n",
    "        \"\"\"Change the perspective of the board state (if necessary).\"\"\"\n",
    "        # This function may need to be defined based on your architecture and how you intend to handle player perspectives.\n",
    "        return state  # No need to modify state representation for this method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ResNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, game, num_resBlock, num_hidden):  # num_hidden is the hidden size of conv blocks\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Updated to accept 13 input channels\n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Conv2d(13, num_hidden, kernel_size=3, padding=1),  # 6 white, 6 black, 1 empty = 13\n",
    "            nn.BatchNorm2d(num_hidden),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.backBone = nn.ModuleList(\n",
    "            [ResBlock(num_hidden) for _ in range(num_resBlock)]  # Using num_hidden for ResBlock\n",
    "        )\n",
    "\n",
    "        self.policyHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 8 * 8, game.action_size),  # Assuming output size is compatible\n",
    "        )\n",
    "\n",
    "        self.valueHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3 * 8 * 8, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.device)  # Move input to the appropriate device (GPU/CPU)\n",
    "        x = self.startBlock(x)  # Pass through the initial block\n",
    "        for res_block in self.backBone:  # Pass through each ResBlock\n",
    "            x = res_block(x)\n",
    "        policy = self.policyHead(x)  # Get policy predictions\n",
    "        value = self.valueHead(x)  # Get value predictions\n",
    "        return policy, value\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
    "\n",
    "    def forward(self, x):  # x is input\n",
    "        residual = x  # Save the input for the skip connection\n",
    "        x = F.relu(self.bn1(self.conv1(x)))  # First convolution + batch norm + ReLU\n",
    "        x = self.bn2(self.conv2(x))  # Second convolution + batch norm\n",
    "        x += residual  # Add the input back for the skip connection\n",
    "        x = F.relu(x)  # ReLU activation after adding residual\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MCTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, game, args, state, parent=None, action_taken=None, prior=0, visit_count=0):\n",
    "        self.game = game  # ChessGame object\n",
    "        self.args = args\n",
    "        self.state = state  # FEN string or any other chess state representation\n",
    "        self.parent = parent\n",
    "        self.action_taken = action_taken\n",
    "        self.prior = prior\n",
    "        \n",
    "        self.children = []\n",
    "        \n",
    "        self.visit_count = visit_count\n",
    "        self.value_sum = 0\n",
    "        \n",
    "    def is_fully_expanded(self):\n",
    "        return len(self.children) > 0\n",
    "    \n",
    "    def select(self):\n",
    "        \"\"\"Select the best child node based on UCB score.\"\"\"\n",
    "        best_child = None\n",
    "        best_ucb = -np.inf\n",
    "        \n",
    "        for child in self.children:\n",
    "            ucb = self.get_ucb(child)\n",
    "            if ucb > best_ucb:\n",
    "                best_child = child\n",
    "                best_ucb = ucb\n",
    "                \n",
    "        return best_child\n",
    "    \n",
    "    def get_ucb(self, child):\n",
    "        \"\"\"Calculate UCB for a child node.\"\"\"\n",
    "        if child.visit_count == 0:\n",
    "            q_value = 0\n",
    "        else:\n",
    "            # Scale value to [0, 1] from [-1, 1]\n",
    "            q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2\n",
    "        return q_value + self.args['C'] * (math.sqrt(self.visit_count) / (child.visit_count + 1)) * child.prior\n",
    "    \n",
    "    def expand(self, policy):\n",
    "        \"\"\"Expand the current node by adding children for each valid move.\"\"\"\n",
    "        for action, prob in enumerate(policy):\n",
    "            if prob > 0:\n",
    "                # Get the next state by applying the action (move)\n",
    "                child_state = self.state  # Using FEN or other format\n",
    "                child_state = self.game.get_next_state(child_state, self.game.index_to_move(action), 1)  # Apply action (chess move)\n",
    "                child_state = self.game.change_perspective(child_state, player=-1)\n",
    "\n",
    "                # Create a new child node and add it to the children list\n",
    "                child = Node(self.game, self.args, child_state, self, action, prob)\n",
    "                self.children.append(child)\n",
    "                \n",
    "        return child  # Return last expanded child node (optional)\n",
    "            \n",
    "    def backpropagate(self, value):\n",
    "        \"\"\"Update the current node and propagate the result back up to the root.\"\"\"\n",
    "        self.value_sum += value\n",
    "        self.visit_count += 1\n",
    "        \n",
    "        # Propagate the value to the parent, changing perspective (opponent's value)\n",
    "        value = self.game.get_opponent_value(value)\n",
    "        if self.parent is not None:\n",
    "            self.parent.backpropagate(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS:\n",
    "    def __init__(self, game, args, model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def search(self, state):\n",
    "        # Initialize the root node of the MCTS tree\n",
    "        root = Node(self.game, self.args, state, visit_count=1)\n",
    "        \n",
    "        # Get the policy and value from the model\n",
    "        policy, _ = self.model(\n",
    "        torch.tensor(self.game.get_encoded_state(state), device=self.model.device).unsqueeze(0).to(self.model.device)\n",
    "        )\n",
    "        \n",
    "        # Apply softmax to policy\n",
    "        policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "        \n",
    "        # Add Dirichlet noise for exploration\n",
    "        policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] \\\n",
    "            * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size)\n",
    "        \n",
    "        # Get valid moves and update policy\n",
    "        valid_moves = self.game.get_valid_moves(state)\n",
    "        policy *= valid_moves\n",
    "        if np.sum(policy)>0:\n",
    "            policy /= np.sum(policy)  # Normalize the policy\n",
    "        else:\n",
    "            policy = valid_moves / np.sum(valid_moves)\n",
    "        # Expand the root node with the computed policy\n",
    "        root.expand(policy)\n",
    "        \n",
    "        # Perform the search for a number of iterations\n",
    "        for search in range(self.args['num_searches']):\n",
    "            node = root\n",
    "            \n",
    "            # Traverse down the tree until an unexpanded node is found\n",
    "            while node.is_fully_expanded():\n",
    "                node = node.select()\n",
    "                \n",
    "            # Get the value and check if the node is terminal\n",
    "            value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
    "            value = self.game.get_opponent_value(value)  # Convert value for the opponent\n",
    "            \n",
    "            # If the node is not terminal, expand further\n",
    "            if not is_terminal:\n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(node.state), device=self.model.device).unsqueeze(0)\n",
    "                )\n",
    "                \n",
    "                policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "                valid_moves = self.game.get_valid_moves(node.state)\n",
    "                \n",
    "                # Update policy based on valid moves\n",
    "                policy *= valid_moves\n",
    "                if np.sum(policy) > 0:\n",
    "                    policy /= np.sum(policy)  # Normalize the policy\n",
    "                else:\n",
    "                    policy = valid_moves / np.sum(valid_moves)\n",
    "                \n",
    "                value = value.item()  # Get the value as a scalar\n",
    "                \n",
    "                # Expand the node with the new policy\n",
    "                node.expand(policy)\n",
    "                \n",
    "            # Backpropagate the value up to the root node\n",
    "            node.backpropagate(value)\n",
    "            \n",
    "        # Collect visit counts from the root's children for action probabilities\n",
    "        action_probs = np.zeros(self.game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.action_taken] = child.visit_count\n",
    "            \n",
    "        action_probs /= np.sum(action_probs)  # Normalize the action probabilities\n",
    "        return action_probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AlphaZero Training loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(game, args, model)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Ensure GPU is set\n",
    "\n",
    "    def selfPlay(self):\n",
    "        memory = []\n",
    "        player = 1\n",
    "        state = self.game.get_initial_state()  # Ensure this returns a 13-channel state\n",
    "\n",
    "        while True:\n",
    "            neutral_state = self.game.change_perspective(state, player)  # This should also return a 13-channel state\n",
    "            action_probs = self.mcts.search(neutral_state)  # Ensure output is compatible\n",
    "\n",
    "            memory.append((self.game.get_encoded_state(neutral_state).to(self.device), action_probs, player))  # Moved to GPU\n",
    "\n",
    "            # Use temperature for exploration\n",
    "            temperature_action_probs = action_probs ** (1 / self.args['temperature'])\n",
    "            if temperature_action_probs.sum() > 0:\n",
    "                temperature_action_probs /= temperature_action_probs.sum()\n",
    "            else:\n",
    "                temperature_action_probs = np.ones_like(temperature_action_probs) / len(temperature_action_probs)  # Handle edge case\n",
    "\n",
    "            action = np.random.choice(self.game.action_size, p=temperature_action_probs)  # Use temperature-adjusted probabilities\n",
    "\n",
    "            uci_move = self.game.index_to_move(action)  # Convert action index to UCI move\n",
    "            state = self.game.get_next_state(state, uci_move, player)  # Ensure this state has 13 channels\n",
    "\n",
    "            value, is_terminal = self.game.get_value_and_terminated(state, action)  # Check termination\n",
    "\n",
    "            if is_terminal:\n",
    "                return_memory = []\n",
    "                for hist_neutral_state, hist_action_probs, hist_player in memory:\n",
    "                    hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
    "                    return_memory.append((\n",
    "                        self.game.get_encoded_state(hist_neutral_state).to(self.device),  # Moved to GPU\n",
    "                        hist_action_probs,\n",
    "                        hist_outcome\n",
    "                    ))\n",
    "                return return_memory\n",
    "\n",
    "            # Switch player\n",
    "            player = self.game.get_opponent(player)\n",
    "\n",
    "    def train(self, memory):\n",
    "        random.shuffle(memory)\n",
    "        for batchIdx in tqdm(range(0, len(memory), self.args['batch_size']), desc=\"Training Batches\"):\n",
    "            sample = memory[batchIdx:min(len(memory), batchIdx + self.args['batch_size'])]\n",
    "            \n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "            state = torch.stack(state).to(self.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32).to(self.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32).view(-1, 1).to(self.device)\n",
    "\n",
    "            out_policy, out_value = self.model(state)\n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Show loss for each batch in tqdm\n",
    "            tqdm.write(f\"Batch Loss = {loss.item()}\")\n",
    "\n",
    "            # Monitor GPU memory\n",
    "            print(f\"Batch Loss = {loss.item()}\")\n",
    "            print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "            print(f\"GPU Memory Cached: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "\n",
    "    def learn(self):\n",
    "        for iteration in range(self.args['num_iterations']):\n",
    "            memory = []\n",
    "            \n",
    "            # Self-play phase\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():  # No need to compute gradients during self-play\n",
    "                for _ in trange(self.args['num_selfPlay_iterations']):\n",
    "                    memory += self.selfPlay()\n",
    "                \n",
    "            # Training phase\n",
    "            self.model.train()\n",
    "            for epoch in trange(self.args['num_epochs']):\n",
    "                self.train(memory)\n",
    "            \n",
    "            # Save model and optimizer states after each iteration\n",
    "            torch.save(self.model.state_dict(), f\"model_{iteration}.pt\")\n",
    "            torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fa0b1b9b7a54ffab47f142bf2711a51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anujm\\AppData\\Local\\Temp\\ipykernel_40280\\1687619928.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(self.game.get_encoded_state(state), device=self.model.device).unsqueeze(0).to(self.model.device)\n",
      "C:\\Users\\anujm\\AppData\\Local\\Temp\\ipykernel_40280\\1687619928.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(self.game.get_encoded_state(node.state), device=self.model.device).unsqueeze(0)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the ChessGame instance\n",
    "chess_game = ChessGame()\n",
    "\n",
    "# Set the device for model training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Initialize the ResNet model for chess with specified parameters\n",
    "model = ResNet(chess_game, num_resBlock=4, num_hidden=64).to(device)  # Ensure model is moved to GPU\n",
    "\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "# Set the arguments for AlphaZero\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 60,\n",
    "    'num_iterations': 3,\n",
    "    'num_selfPlay_iterations': 500,\n",
    "    'num_parallel_games': 100,\n",
    "    'num_epochs': 4,\n",
    "    'batch_size': 64,  # This will manage GPU memory by processing in batches\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "# Create an instance of AlphaZero for chess\n",
    "alphaZero = AlphaZero(model, optimizer, chess_game, args)\n",
    "\n",
    "# Start the learning process\n",
    "alphaZero.learn()  # This now runs on the GPU with batching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt-v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
