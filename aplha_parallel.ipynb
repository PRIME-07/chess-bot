{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 4080 Laptop GPU is available.\n",
      "cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed874416abdd419eb1ad8bb0413da5c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anujm\\AppData\\Local\\Temp\\ipykernel_3864\\2920192007.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(self.game.get_encoded_state(node.state), device=self.model.device).unsqueeze(0)\n",
      "C:\\Users\\anujm\\AppData\\Local\\Temp\\ipykernel_3864\\2920192007.py:294: RuntimeWarning: invalid value encountered in divide\n",
      "  policy = valid_moves / np.sum(valid_moves)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No valid moves were found in the policy. Check if the policy is correctly generated.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 478\u001b[0m\n\u001b[0;32m    475\u001b[0m alphaZero_parallel \u001b[38;5;241m=\u001b[39m AlphaZero_parallel(model, optimizer, chess_game, args)\n\u001b[0;32m    477\u001b[0m \u001b[38;5;66;03m# Start the learning process in parallel\u001b[39;00m\n\u001b[1;32m--> 478\u001b[0m \u001b[43malphaZero_parallel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# This will run the learning process in parallel using multiple games\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 431\u001b[0m, in \u001b[0;36mAlphaZero_parallel.learn_parallel\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# No need to compute gradients during self-play\u001b[39;00m\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_selfPlay_iterations\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m--> 431\u001b[0m         memory \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselfPlay_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;66;03m# Training phase\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n",
      "Cell \u001b[1;32mIn[4], line 364\u001b[0m, in \u001b[0;36mAlphaZero_parallel.selfPlay_parallel\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;66;03m# Change perspective for all players and get action probabilities\u001b[39;00m\n\u001b[0;32m    363\u001b[0m     neutral_state_batch \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mchange_perspective(state, player) \u001b[38;5;28;01mfor\u001b[39;00m state, player \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(state_batch, player_batch)]\n\u001b[1;32m--> 364\u001b[0m     action_probs_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmcts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneutral_state_batch\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Run parallelized MCTS search\u001b[39;00m\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_parallel_games\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m    367\u001b[0m         memory_batch\u001b[38;5;241m.\u001b[39mappend((\n\u001b[0;32m    368\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mget_encoded_state(neutral_state_batch[i])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[0;32m    369\u001b[0m             action_probs_batch[i],\n\u001b[0;32m    370\u001b[0m             player_batch[i]\n\u001b[0;32m    371\u001b[0m         ))\n",
      "File \u001b[1;32mc:\\Users\\anujm\\anaconda3\\envs\\pyt-v1\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[4], line 296\u001b[0m, in \u001b[0;36mMCTS_parallel.search\u001b[1;34m(self, state_batch)\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    294\u001b[0m         policy \u001b[38;5;241m=\u001b[39m valid_moves \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(valid_moves)\n\u001b[1;32m--> 296\u001b[0m     \u001b[43mroot_nodes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;66;03m# Perform MCTS search for each game in parallel\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m search \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_searches\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n",
      "Cell \u001b[1;32mIn[4], line 247\u001b[0m, in \u001b[0;36mNode.expand\u001b[1;34m(self, policy)\u001b[0m\n\u001b[0;32m    244\u001b[0m         last_child \u001b[38;5;241m=\u001b[39m child  \u001b[38;5;66;03m# Track the last valid child created\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m last_child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo valid moves were found in the policy. Check if the policy is correctly generated.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m last_child\n",
      "\u001b[1;31mValueError\u001b[0m: No valid moves were found in the policy. Check if the policy is correctly generated."
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(0)\n",
    "from tqdm.notebook import trange\n",
    "import random\n",
    "import math\n",
    "import chess\n",
    "import chess.engine\n",
    "import matplotlib as plt \n",
    "from tqdm import tqdm\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "else:\n",
    "    print(\"No GPU available. Training will run on CPU.\")\n",
    "\n",
    "\n",
    "# CHESS_GAME\n",
    "\n",
    "import chess\n",
    "import numpy as np\n",
    "\n",
    "class ChessGame:\n",
    "    def __init__(self, device):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.board = chess.Board()\n",
    "        self.action_size = 24710  # Max possible moves incl. promotions\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        return self.board.fen()  # FEN is a standard notation for representing the current state of the chess board\n",
    "    \n",
    "    def get_next_state(self, state, action, player):\n",
    "        \"\"\"Update the board with the chosen action.\"\"\" \n",
    "        board = chess.Board(state)\n",
    "        move = chess.Move.from_uci(action)\n",
    "        if move in board.legal_moves:  # Check if the move is legal\n",
    "            board.push(move)\n",
    "        else:\n",
    "            raise ValueError(f\"Illegal move: {action}\")\n",
    "        return board.fen()\n",
    "    \n",
    "    def get_valid_moves(self, state):\n",
    "        \"\"\"Get a binary mask of valid moves.\"\"\" \n",
    "        board = chess.Board(state)\n",
    "        legal_moves = list(board.legal_moves)\n",
    "        valid_moves = np.zeros(self.action_size, dtype=np.uint8)\n",
    "        for move in legal_moves:\n",
    "            move_idx = self.move_to_index(move)\n",
    "            valid_moves[move_idx] = 1\n",
    "        return valid_moves\n",
    "    \n",
    "    def get_opponent(self, player):\n",
    "        \"\"\"Toggle between players (1 for white, -1 for black).\"\"\"\n",
    "        return -player\n",
    "    \n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "\n",
    "    def check_win(self, state, action):\n",
    "        board = chess.Board(state)\n",
    "        return board.is_game_over()\n",
    "    \n",
    "    def get_value_and_terminated(self, state, action):\n",
    "        board = chess.Board(state)\n",
    "        if board.is_checkmate():\n",
    "            return 1, True\n",
    "        elif board.is_stalemate() or board.is_insufficient_material():\n",
    "            return 0, True\n",
    "        return 0, False\n",
    "    \n",
    "    def get_encoded_state(self, state):\n",
    "        \"\"\"Encode the board state into a tensor format for input to the neural network.\"\"\"\n",
    "        board = chess.Board(state)\n",
    "        board_tensor = np.zeros((13, 8, 8), dtype=np.float32)  # Shape [13, 8, 8]\n",
    "        piece_map = board.piece_map()\n",
    "        \n",
    "        for square, piece in piece_map.items():\n",
    "            piece_type = piece.piece_type - 1 if piece.color else piece.piece_type + 5 \n",
    "            board_tensor[piece_type, square // 8, square % 8] = 1 \n",
    "            \n",
    "        # Set empty squares to the 12th channel\n",
    "        for row in range(8):\n",
    "            for col in range(8):\n",
    "                if board_tensor[:, row, col].sum() == 0:\n",
    "                    board_tensor[12, row, col] = 1  # Mark as empty\n",
    "        \n",
    "        # Convert to torch tensor and move to device\n",
    "        board_tensor = torch.FloatTensor(board_tensor).to(device)\n",
    "    \n",
    "        return board_tensor\n",
    "\n",
    "    \n",
    "    def move_to_index(self, move):\n",
    "        \"\"\"Convert a move to a unique index for the action space.\"\"\" \n",
    "        uci_move = move.uci()\n",
    "        from_square = chess.SQUARE_NAMES.index(uci_move[:2])\n",
    "        to_square = chess.SQUARE_NAMES.index(uci_move[2:4])\n",
    "        promotion = move.promotion or 0\n",
    "        return from_square * 64 + to_square + promotion * 64 * 64\n",
    "\n",
    "    def index_to_move(self, index):\n",
    "        \"\"\"Convert an index back to a move.\"\"\" \n",
    "        promotion = index // (64 * 64)\n",
    "        index %= (64 * 64)\n",
    "        from_square = index // 64\n",
    "        to_square = index % 64\n",
    "        move = chess.Move(from_square, to_square, promotion)\n",
    "        return move.uci()\n",
    "    \n",
    "    def change_perspective(self, state, player):\n",
    "        \"\"\"Change the perspective of the board state (if necessary).\"\"\"\n",
    "        # This function may need to be defined based on your architecture and how you intend to handle player perspectives.\n",
    "        return state  # No need to modify state representation for this method\n",
    "\n",
    "\n",
    "\n",
    "# RESNET\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, game, num_resBlock, num_hidden):  # num_hidden is the hidden size of conv blocks\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Updated to accept 13 input channels\n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Conv2d(13, num_hidden, kernel_size=3, padding=1),  # 6 white, 6 black, 1 empty = 13\n",
    "            nn.BatchNorm2d(num_hidden),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.backBone = nn.ModuleList(\n",
    "            [ResBlock(num_hidden) for _ in range(num_resBlock)]  # Using num_hidden for ResBlock\n",
    "        )\n",
    "\n",
    "        self.policyHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 8 * 8, game.action_size),  # Assuming output size is compatible\n",
    "        )\n",
    "\n",
    "        self.valueHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3 * 8 * 8, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.device)  # Move input to the appropriate device (GPU/CPU)\n",
    "        x = self.startBlock(x)  # Pass through the initial block\n",
    "        for res_block in self.backBone:  # Pass through each ResBlock\n",
    "            x = res_block(x)\n",
    "        policy = self.policyHead(x)  # Get policy predictions\n",
    "        value = self.valueHead(x)  # Get value predictions\n",
    "        return policy, value\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
    "\n",
    "    def forward(self, x):  # x is input\n",
    "        residual = x  # Save the input for the skip connection\n",
    "        x = F.relu(self.bn1(self.conv1(x)))  # First convolution + batch norm + ReLU\n",
    "        x = self.bn2(self.conv2(x))  # Second convolution + batch norm\n",
    "        x += residual  # Add the input back for the skip connection\n",
    "        x = F.relu(x)  # ReLU activation after adding residual\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# MCTS\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, game, args, state, parent=None, action_taken=None, prior=0, visit_count=0):\n",
    "        self.game = game  # ChessGame object\n",
    "        self.args = args\n",
    "        self.state = state  # FEN string or any other chess state representation\n",
    "        self.parent = parent\n",
    "        self.action_taken = action_taken\n",
    "        self.prior = prior\n",
    "        \n",
    "        self.children = []\n",
    "        \n",
    "        self.visit_count = visit_count\n",
    "        self.value_sum = 0\n",
    "        \n",
    "    def is_fully_expanded(self):\n",
    "        return len(self.children) > 0\n",
    "    \n",
    "    def select(self):\n",
    "        \"\"\"Select the best child node based on UCB score.\"\"\"\n",
    "        best_child = None\n",
    "        best_ucb = -np.inf\n",
    "        \n",
    "        for child in self.children:\n",
    "            ucb = self.get_ucb(child)\n",
    "            if ucb > best_ucb:\n",
    "                best_child = child\n",
    "                best_ucb = ucb\n",
    "                \n",
    "        return best_child\n",
    "    \n",
    "    def get_ucb(self, child):\n",
    "        \"\"\"Calculate UCB for a child node.\"\"\"\n",
    "        if child.visit_count == 0:\n",
    "            q_value = 0\n",
    "        else:\n",
    "            # Scale value to [0, 1] from [-1, 1]\n",
    "            q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2\n",
    "        return q_value + self.args['C'] * (math.sqrt(self.visit_count) / (child.visit_count + 1)) * child.prior\n",
    "    \n",
    "    def expand(self, policy):\n",
    "        \"\"\"Expand the current node by adding children for each valid move.\"\"\"\n",
    "        last_child = None  # Initialize to None to handle no valid move case\n",
    "        for action, prob in enumerate(policy):\n",
    "            if prob > 0:\n",
    "                # Get the next state by applying the action (move)\n",
    "                child_state = self.state  # Using FEN or other format\n",
    "                child_state = self.game.get_next_state(child_state, self.game.index_to_move(action), 1)  # Apply action (chess move)\n",
    "                child_state = self.game.change_perspective(child_state, player=-1)\n",
    "\n",
    "                # Create a new child node and add it to the children list\n",
    "                child = Node(self.game, self.args, child_state, self, action, prob)\n",
    "                self.children.append(child)\n",
    "                last_child = child  # Track the last valid child created\n",
    "\n",
    "        if last_child is None:\n",
    "            raise ValueError(\"No valid moves were found in the policy. Check if the policy is correctly generated.\")\n",
    "\n",
    "        return last_child  # Return last valid expanded child node\n",
    "\n",
    "            \n",
    "    def backpropagate(self, value):\n",
    "        \"\"\"Update the current node and propagate the result back up to the root.\"\"\"\n",
    "        self.value_sum += value\n",
    "        self.visit_count += 1\n",
    "        \n",
    "        # Propagate the value to the parent, changing perspective (opponent's value)\n",
    "        value = self.game.get_opponent_value(value)\n",
    "        if self.parent is not None:\n",
    "            self.parent.backpropagate(value)\n",
    "\n",
    "\n",
    "class MCTS_parallel:\n",
    "    def __init__(self, game, args, model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def search(self, state_batch):\n",
    "        # We assume state_batch is a batch of game states, and we're going to parallelize the MCTS searches for each.\n",
    "        batch_size = len(state_batch)\n",
    "        root_nodes = [Node(self.game, self.args, state, visit_count=1) for state in state_batch]\n",
    "        \n",
    "        # Get policy and value from the model for each state in the batch\n",
    "        encoded_states = torch.stack([self.game.get_encoded_state(state) for state in state_batch]).to(self.model.device)\n",
    "        policy_batch, _ = self.model(encoded_states)\n",
    "        \n",
    "        # Apply softmax to the policies for each state\n",
    "        policy_batch = torch.softmax(policy_batch, axis=1).cpu().numpy()\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            # Apply Dirichlet noise for exploration\n",
    "            policy = (1 - self.args['dirichlet_epsilon']) * policy_batch[i] + self.args['dirichlet_epsilon'] \\\n",
    "                * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size)\n",
    "            \n",
    "            # Get valid moves and update policy for each game in the batch\n",
    "            valid_moves = self.game.get_valid_moves(state_batch[i])\n",
    "            policy *= valid_moves\n",
    "            \n",
    "            if np.sum(policy) > 0:\n",
    "                policy /= np.sum(policy)  # Normalize the policy\n",
    "            else:\n",
    "                policy = valid_moves / np.sum(valid_moves)\n",
    "            \n",
    "            root_nodes[i].expand(policy)\n",
    "        \n",
    "        # Perform MCTS search for each game in parallel\n",
    "        for search in range(self.args['num_searches']):\n",
    "            for root in root_nodes:\n",
    "                node = root\n",
    "                \n",
    "                # Traverse down the tree until an unexpanded node is found\n",
    "                while node.is_fully_expanded():\n",
    "                    node = node.select()\n",
    "                \n",
    "                value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
    "                value = self.game.get_opponent_value(value)  # Convert value for the opponent\n",
    "                \n",
    "                if not is_terminal:\n",
    "                    policy, value = self.model(\n",
    "                        torch.tensor(self.game.get_encoded_state(node.state), device=self.model.device).unsqueeze(0)\n",
    "                    )\n",
    "                    policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "                    valid_moves = self.game.get_valid_moves(node.state)\n",
    "                    \n",
    "                    # Update policy based on valid moves\n",
    "                    policy *= valid_moves\n",
    "                    if np.sum(policy) > 0:\n",
    "                        policy /= np.sum(policy)  # Normalize the policy\n",
    "                    else:\n",
    "                        policy = valid_moves / np.sum(valid_moves)\n",
    "                    \n",
    "                    value = value.item()  # Get the value as a scalar\n",
    "                    node.expand(policy)\n",
    "                \n",
    "                node.backpropagate(value)\n",
    "        \n",
    "        # Collect action probabilities for each game in the batch\n",
    "        action_probs_batch = np.zeros((batch_size, self.game.action_size))\n",
    "        for i, root in enumerate(root_nodes):\n",
    "            for child in root.children:\n",
    "                action_probs_batch[i][child.action_taken] = child.visit_count\n",
    "        \n",
    "        # Normalize action probabilities for each game in the batch\n",
    "        for i in range(batch_size):\n",
    "            action_probs_batch[i] /= np.sum(action_probs_batch[i])\n",
    "        \n",
    "        return action_probs_batch\n",
    "\n",
    "\n",
    "\n",
    "#ALPHA_ZERO_PARALLEL\n",
    "\n",
    "class AlphaZero_parallel:\n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTS_parallel(game, args, model)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Ensure GPU is set\n",
    "\n",
    "    def selfPlay_parallel(self):\n",
    "        \"\"\"Run multiple games in parallel.\"\"\"\n",
    "        memory_batch = []\n",
    "        player_batch = [1] * self.args['num_parallel_games']  # All players start as white\n",
    "        state_batch = [self.game.get_initial_state() for _ in range(self.args['num_parallel_games'])]  # Initial state for each game\n",
    "        \n",
    "        # Continue the games until all have reached a terminal state\n",
    "        while True:\n",
    "            # Change perspective for all players and get action probabilities\n",
    "            neutral_state_batch = [self.game.change_perspective(state, player) for state, player in zip(state_batch, player_batch)]\n",
    "            action_probs_batch = self.mcts.search(neutral_state_batch)  # Run parallelized MCTS search\n",
    "\n",
    "            for i in range(self.args['num_parallel_games']):\n",
    "                memory_batch.append((\n",
    "                    self.game.get_encoded_state(neutral_state_batch[i]).to(self.device),\n",
    "                    action_probs_batch[i],\n",
    "                    player_batch[i]\n",
    "                ))\n",
    "\n",
    "            # Use temperature for exploration\n",
    "            for i in range(self.args['num_parallel_games']):\n",
    "                temperature_action_probs = action_probs_batch[i] ** (1 / self.args['temperature'])\n",
    "                temperature_action_probs /= np.sum(temperature_action_probs)\n",
    "                \n",
    "                action = np.random.choice(self.game.action_size, p=temperature_action_probs)\n",
    "                uci_move = self.game.index_to_move(action)\n",
    "                state_batch[i] = self.game.get_next_state(state_batch[i], uci_move, player_batch[i])\n",
    "\n",
    "            # Check for terminal states and store outcomes\n",
    "            is_terminal_batch = [self.game.get_value_and_terminated(state, action)[1] for state, action in zip(state_batch, action_probs_batch)]\n",
    "\n",
    "            if all(is_terminal_batch):\n",
    "                return_memory = []\n",
    "                for hist_neutral_state, hist_action_probs, hist_player in memory_batch:\n",
    "                    hist_outcome = self.game.get_value_and_terminated(state_batch[0], action_probs_batch[0])[0]\n",
    "                    return_memory.append((\n",
    "                        hist_neutral_state,\n",
    "                        hist_action_probs,\n",
    "                        hist_outcome\n",
    "                    ))\n",
    "                return return_memory\n",
    "\n",
    "            # Switch players\n",
    "            player_batch = [self.game.get_opponent(player) for player in player_batch]\n",
    "\n",
    "    def train(self, memory):\n",
    "        \"\"\"Training process remains the same as in the original class.\"\"\"\n",
    "        random.shuffle(memory)\n",
    "        for batchIdx in tqdm(range(0, len(memory), self.args['batch_size']), desc=\"Training Batches\"):\n",
    "            sample = memory[batchIdx:min(len(memory), batchIdx + self.args['batch_size'])]\n",
    "            \n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "            state = torch.stack(state).to(self.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32).to(self.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32).view(-1, 1).to(self.device)\n",
    "\n",
    "            out_policy, out_value = self.model(state)\n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Show loss for each batch in tqdm\n",
    "            tqdm.write(f\"Batch Loss = {loss.item()}\")\n",
    "\n",
    "    def learn_parallel(self):\n",
    "        \"\"\"Learn using parallelized self-play.\"\"\"\n",
    "        for iteration in range(self.args['num_iterations']):\n",
    "            memory = []\n",
    "            \n",
    "            # Self-play phase\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():  # No need to compute gradients during self-play\n",
    "                for _ in trange(self.args['num_selfPlay_iterations']):\n",
    "                    memory += self.selfPlay_parallel()\n",
    "                \n",
    "            # Training phase\n",
    "            self.model.train()\n",
    "            for epoch in trange(self.args['num_epochs']):\n",
    "                self.train(memory)\n",
    "            \n",
    "            # Save model and optimizer states after each iteration\n",
    "            torch.save(self.model.state_dict(), f\"model_parallel_{iteration}.pt\")\n",
    "            torch.save(self.optimizer.state_dict(), f\"optimizer_parallel_{iteration}.pt\")\n",
    "\n",
    "\n",
    "\n",
    "# MAIN\n",
    "\n",
    "# Initialize the ChessGame instance\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "chess_game = ChessGame(device=device)\n",
    "\n",
    "# Set the device for model training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Initialize the ResNet model for chess with specified parameters\n",
    "model = ResNet(chess_game, num_resBlock=4, num_hidden=64).to(device)  # Ensure model is moved to GPU\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "# Set the arguments for AlphaZero_parallel\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 5,  # Adjust this according to the computing power\n",
    "    'num_iterations': 5,\n",
    "    'num_selfPlay_iterations': 5,  # You can increase this for full training\n",
    "    'num_parallel_games': 5,  # Adjust the number of parallel games (for testing, keep it low)\n",
    "    'num_epochs': 4,\n",
    "    'batch_size': 1,  # Manage GPU memory by processing in batches\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "# Create an instance of AlphaZero_parallel for chess\n",
    "alphaZero_parallel = AlphaZero_parallel(model, optimizer, chess_game, args)\n",
    "\n",
    "# Start the learning process in parallel\n",
    "alphaZero_parallel.learn_parallel()  # This will run the learning process in parallel using multiple games\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt-v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
